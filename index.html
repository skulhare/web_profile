<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sourabh Kulhare - AI Research Engineer</title>
  <style>
    * {
      scroll-behavior: smooth;
    }

    body {
      font-family: 'Segoe UI', sans-serif;
      background: #f4f4f4;
      color: #333;
      margin: 0;
      padding: 0;
      line-height: 1.6;
    }

    header {
      background: #003f5c;
      color: white;
      padding: 2rem;
      text-align: center;
      position: relative;
    }

    .profile-photo {
      width: 150px;
      height: 150px;
      border-radius: 50%;
      border: 4px solid #90e0ef;
      margin: 1rem auto 1rem auto;
      display: block;
      object-fit: cover;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
      transition: transform 0.3s ease;
    }

    .profile-photo:hover {
      transform: scale(1.05);
    }

    nav {
      margin-top: 1rem;
    }

    nav a {
      color: #90e0ef;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
      padding: 8px 12px;
      border-radius: 20px;
      transition: all 0.3s ease;
      display: inline-block;
    }

    nav a:hover {
      background: rgba(144, 224, 239, 0.2);
      transform: translateY(-2px);
    }

    section {
      padding: 2rem;
      max-width: 900px;
      margin: auto;
      background: white;
      margin-top: 1rem;
      border-radius: 10px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    section:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 16px rgba(0,0,0,0.15);
    }

    h2 {
      color: #0077b6;
      border-bottom: 2px solid #90e0ef;
      padding-bottom: 0.5rem;
      margin-bottom: 1.5rem;
    }

    ul {
      padding-left: 20px;
    }

    footer {
      text-align: center;
      padding: 1rem;
      background: #003f5c;
      color: white;
      margin-top: 2rem;
    }

    a {
      color: #0077b6;
      transition: color 0.3s ease;
    }

    a:hover {
      color: #005577;
    }

    .resume-btn {
      background: #0077b6;
      color: white;
      padding: 10px 20px;
      text-decoration: none;
      border-radius: 5px;
      display: inline-block;
      margin: 10px 0;
      font-weight: bold;
      transition: all 0.3s ease;
    }

    .resume-btn:hover {
      background: #005577;
      color: white;
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0,119,182,0.3);
    }

    .resume-btn-header {
      background: #90e0ef;
      color: #003f5c;
      padding: 8px 16px;
      text-decoration: none;
      border-radius: 20px;
      display: inline-block;
      margin-top: 10px;
      font-weight: bold;
      transition: all 0.3s ease;
    }

    .resume-btn-header:hover {
      background: white;
      color: #003f5c;
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(144, 224, 239, 0.4);
    }

    /* Skills Section Enhancements */
    .skill-category {
      margin-bottom: 1.5rem;
      padding: 1rem;
      background: #f8f9fa;
      border-radius: 8px;
      border-left: 4px solid #0077b6;
    }

    .skill-item {
      display: flex;
      align-items: center;
      margin: 0.5rem 0;
    }

    .skill-icon {
      font-size: 1.2rem;
      margin-right: 0.8rem;
      width: 24px;
      text-align: center;
    }

    .skill-name {
      flex: 1;
      font-weight: 500;
    }

    /* Project Enhancement */
    .project-item {
      background: #f8f9fa;
      padding: 1.5rem;
      margin: 1rem 0;
      border-radius: 8px;
      border-left: 4px solid #0077b6;
      transition: all 0.3s ease;
    }

    .project-item:hover {
      background: #e9ecef;
      transform: translateX(5px);
    }

    .project-title {
      font-weight: bold;
      color: #0077b6;
      margin-bottom: 0.5rem;
    }

    .project-description {
      margin-bottom: 0.8rem;
      color: #555;
    }

    .tech-stack {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
    }

    .tech-tag {
      background: #0077b6;
      color: white;
      padding: 0.3rem 0.6rem;
      border-radius: 15px;
      font-size: 0.8rem;
      font-weight: 500;
    }

    /* Back to Top Button */
    .back-to-top {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background: #0077b6;
      color: white;
      border: none;
      border-radius: 50%;
      width: 50px;
      height: 50px;
      cursor: pointer;
      opacity: 0;
      visibility: hidden;
      transition: all 0.3s ease;
      font-size: 1.2rem;
      box-shadow: 0 4px 12px rgba(0,119,182,0.3);
    }

    .back-to-top.visible {
      opacity: 1;
      visibility: visible;
    }

    .back-to-top:hover {
      background: #005577;
      transform: translateY(-2px);
    }

    /* Mobile Responsiveness */
    @media (max-width: 768px) {
      header {
        padding: 1.5rem 1rem;
      }

      .profile-photo {
        width: 120px;
        height: 120px;
      }

      nav {
        margin-top: 1rem;
      }

      nav a {
        display: block;
        margin: 0.5rem auto;
        text-align: center;
        max-width: 200px;
      }

      section {
        margin: 1rem;
        padding: 1.5rem;
      }

      h1 {
        font-size: 1.8rem;
      }

      h2 {
        font-size: 1.5rem;
      }

      .tech-stack {
        gap: 0.3rem;
      }

      .tech-tag {
        font-size: 0.7rem;
        padding: 0.2rem 0.5rem;
      }

      .skill-item {
        flex-direction: column;
        align-items: flex-start;
        text-align: left;
      }

      .skill-icon {
        margin-bottom: 0.3rem;
      }
    }

    @media (max-width: 480px) {
      header {
        padding: 1rem 0.5rem;
      }

      .profile-photo {
        width: 100px;
        height: 100px;
      }

      section {
        margin: 0.5rem;
        padding: 1rem;
      }

      h1 {
        font-size: 1.5rem;
      }

      .resume-btn, .resume-btn-header {
        width: 90%;
        text-align: center;
        margin: 0.5rem auto;
        display: block;
      }

      .project-item {
        padding: 1rem;
      }
    }
  </style>
</head>
<body>

<header>
  <img src="supporting_files/IMG_5192.jpg" alt="Sourabh Kulhare" class="profile-photo">
  <h1>Sourabh Kulhare</h1>
  <p>AI Research Engineer | Deep Learning | Computer Vision | Generative Models</p>
  <a href="supporting_files/Sourabh-KulhareJune25.pdf" download class="resume-btn-header">üìÑ Download Resume</a>
  <nav>
    <a href="#about">About</a>
    <a href="#experience">Experience</a>
    <a href="#education">Education</a>
    <a href="#skills">Skills</a>
    <a href="#projects">Projects</a>
    <a href="#publications">Publications</a>
    <a href="#talks">Talks</a>
    <a href="#contact">Contact</a>
  </nav>
</header>

<section id="about">
  <h2>About Me</h2>
  <p>
    AI Research Engineer with 8+ years of experience in deep learning, LLMs, generative models, and edge deployment. 
    Proven track record leading AI solutions from research to FDA-submittable technology, with publications in top conferences (CVPR, MICCAI). 
    Passionate about transforming real-world challenges into scalable ML solutions.
  </p>
</section>

<section id="experience">
  <h2>Experience</h2>
  <ul>
    <li><strong>Machine Learning Research Engineer</strong> ‚Äì Global Health Labs, Bellevue, WA (2020‚ÄìPresent)</li>
    <li><strong>Associate Machine Learning Scientist</strong> ‚Äì Intellectual Ventures Laboratory, Bellevue, WA (2017‚Äì2020)</li>
    <li><strong>Deep Learning Graduate Researcher</strong> ‚Äì RIT (2016‚Äì2017), Rochester, NY</li>
    <li><strong>Computer Vision and NLP Intern</strong> ‚Äì Ahold - USA (2016), Boston, MA</li>
  </ul>
</section>

<section id="education">
  <h2>Education</h2>
  <ul>
    <li>
      <strong>Master of Science in Computer Engineering</strong><br>
      Rochester Institute of Technology, Rochester, NY<br>
      <em>2014‚Äì2017</em>
    </li>
    <li>
      <strong>Bachelor of Technology in Electronics and Communications</strong><br>
      Lovely Professional University, Jalandhar, India<br>
      <em>2009‚Äì2013</em>
    </li>
  </ul>
</section>

<section id="skills">
  <h2>Skills & Tools</h2>
  
  <div class="skill-category">
    <h3>Programming Languages</h3>
    <div class="skill-item">
      <span class="skill-icon">üêç</span>
      <span class="skill-name">Python</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">‚ö°</span>
      <span class="skill-name">C++</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">‚òï</span>
      <span class="skill-name">Java</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üåô</span>
      <span class="skill-name">Lua</span>
    </div>
  </div>

  <div class="skill-category">
    <h3>ML/DL Frameworks</h3>
    <div class="skill-item">
      <span class="skill-icon">üî•</span>
      <span class="skill-name">PyTorch</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üß†</span>
      <span class="skill-name">TensorFlow</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üìä</span>
      <span class="skill-name">scikit-learn</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üéØ</span>
      <span class="skill-name">Keras</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üëÅÔ∏è</span>
      <span class="skill-name">OpenCV</span>
    </div>
  </div>

  <div class="skill-category">
    <h3>LLMs & NLP</h3>
    <div class="skill-item">
      <span class="skill-icon">üîó</span>
      <span class="skill-name">LangChain</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üîç</span>
      <span class="skill-name">RAG (Retrieval-Augmented Generation)</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">ü§ó</span>
      <span class="skill-name">Hugging Face</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üìù</span>
      <span class="skill-name">NLTK</span>
    </div>
  </div>

  <div class="skill-category">
    <h3>Deployment & Cloud</h3>
    <div class="skill-item">
      <span class="skill-icon">üåê</span>
      <span class="skill-name">Flask</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üê≥</span>
      <span class="skill-name">Docker</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">‚ö°</span>
      <span class="skill-name">TensorRT</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üì±</span>
      <span class="skill-name">TFLite</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">‚òÅÔ∏è</span>
      <span class="skill-name">AWS</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üîµ</span>
      <span class="skill-name">Azure</span>
    </div>
  </div>

  <div class="skill-category">
    <h3>Tools & Other</h3>
    <div class="skill-item">
      <span class="skill-icon">üìà</span>
      <span class="skill-name">W&B (Weights & Biases)</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üîß</span>
      <span class="skill-name">Git/GitHub</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üóÑÔ∏è</span>
      <span class="skill-name">SQL</span>
    </div>
    <div class="skill-item">
      <span class="skill-icon">üé®</span>
      <span class="skill-name">Generative Models (GANs, Diffusion)</span>
    </div>
  </div>
</section>

<section id="projects">
  <h2>Projects</h2>
  
  <div class="project-item">
    <div class="project-title">Self-supervised Vision Foundation Model for Video Analysis</div>
    <div class="project-description">
      Developed a self-supervised learning framework for video understanding using foundation models, enabling robust feature extraction from unlabeled video data for downstream tasks.
    </div>
    <div class="tech-stack">
      <span class="tech-tag">PyTorch</span>
      <span class="tech-tag">Computer Vision</span>
      <span class="tech-tag">Self-supervised Learning</span>
      <span class="tech-tag">Video Processing</span>
    </div>
  </div>

  <div class="project-item">
    <div class="project-title">State-Space Models (MAMBA) - Transformer Alternative</div>
    <div class="project-description">
      Implemented and optimized MAMBA architecture as an efficient replacement for Transformers, achieving better performance with linear complexity for long sequences.
    </div>
    <div class="tech-stack">
      <span class="tech-tag">PyTorch</span>
      <span class="tech-tag">MAMBA</span>
      <span class="tech-tag">State-Space Models</span>
      <span class="tech-tag">Deep Learning</span>
      <span class="tech-tag">NLP</span>
    </div>
  </div>

  <div class="project-item">
    <div class="project-title">Scientific Journal Classification with Fine-tuned Mini LLM</div>
    <div class="project-description">
      Built an efficient text classification system using a fine-tuned lightweight LLM to categorize scientific papers by research domain and methodology.
    </div>
    <div class="tech-stack">
      <span class="tech-tag">Hugging Face</span>
      <span class="tech-tag">Fine-tuning</span>
      <span class="tech-tag">Text Classification</span>
      <span class="tech-tag">LLM</span>
      <span class="tech-tag">Scientific Computing</span>
    </div>
  </div>

  <div class="project-item">
    <div class="project-title">Stock Prediction with LLMs and Financial News</div>
    <div class="project-description">
      Developed a stock prediction system using fine-tuned LLMs to analyze financial news sentiment and market trends with LangChain and RAG architecture.
    </div>
    <div class="tech-stack">
      <span class="tech-tag">LangChain</span>
      <span class="tech-tag">RAG</span>
      <span class="tech-tag">Financial Analysis</span>
      <span class="tech-tag">LLM</span>
      <span class="tech-tag">Sentiment Analysis</span>
    </div>
  </div>

  <div class="project-item">
    <div class="project-title">TB Prediction from Cough Audio Analysis</div>
    <div class="project-description">
      Created an audio-based tuberculosis detection system using MFCC features, LSTM networks, and Multiple Instance Learning for early disease diagnosis.
    </div>
    <div class="tech-stack">
      <span class="tech-tag">Audio Processing</span>
      <span class="tech-tag">MFCC</span>
      <span class="tech-tag">LSTM</span>
      <span class="tech-tag">MIL</span>
      <span class="tech-tag">Healthcare AI</span>
    </div>
  </div>

  <div class="project-item">
    <div class="project-title">Image Quality Filter with YOLO and DINOv2</div>
    <div class="project-description">
      Built an automated image quality assessment system combining YOLOv7 object detection with DINOv2 vision transformer for content quality evaluation.
    </div>
    <div class="tech-stack">
      <span class="tech-tag">YOLOv7</span>
      <span class="tech-tag">DINOv2</span>
      <span class="tech-tag">Computer Vision</span>
      <span class="tech-tag">Image Processing</span>
      <span class="tech-tag">Quality Assessment</span>
    </div>
  </div>
</section>

<section id="publications">
  <h2>Publications</h2>
  <ul>
    <li>
      <strong>Title:</strong> Deep Learning for Pneumonia Detection in Chest Ultrasound<br>
      <strong>Venue:</strong> CVPR 2023, Vancouver, Canada<br>
      <strong>Description:</strong> This paper presents deep learning algorithms that accurately detect key lung abnormalities‚Äîpleural effusion, consolidation, and B-lines‚Äîin ultrasound scans, using a large dataset from patients in Nigeria and China. These lightweight models, suitable for mobile deployment, aim to assist point-of-care pneumonia diagnosis in low- and middle-income countries, where expert interpretation is scarce. The study also addresses challenges in defining ground truth for pneumonia diagnosis in such settings.<br>
      <strong>Link:</strong> <a href="https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/papers/Shea_Deep_Learning_Video_Classification_of_Lung_Ultrasound_Features_Associated_With_CVPRW_2023_paper.pdf" target="_blank">View Paper</a>
    </li>
    <li>
      <strong>Title:</strong> Medical Image Synthesis Using Generative Adversarial Networks<br>
      <strong>Venue:</strong> MICCAI 2023, Vancouver, Canada<br>
      <strong>Description:</strong> Training deep learning models in healthcare is limited by data scarcity and privacy concerns; this study shows that incorporating synthetic data from generative models improves performance, balances datasets, and protects privacy, outperforming models trained on real data alone.<br>
      <strong>Link:</strong> <a href="https://arxiv.org/pdf/2310.03608" target="_blank">View Paper</a>
    </li>
    <li>
      <strong>Title:</strong> Lung Abnormality Detection in Medical Imaging Using Efficient CNNs<br>
      <strong>Venue:</strong> MICCAI 2018, Granada, Spain<br>
      <strong>Description:</strong> Ultrasound can detect various lung conditions, but interpreting its non-structural features requires expertise. This study presents a CNN-based algorithm trained on annotated ultrasound videos from swine models to identify five key lung features, achieving accurate detection‚Äîincluding pneumothorax using simulated M-mode inputs.<br>
      <strong>Link:</strong> <a href="https://www.researchgate.net/profile/Petrov-Igor/publication/358324752_Inclusion_imaging_using_single-shot_ultrasound_and_convolutional_neural_networks/links/668e756f3e0edb1e0fdb4931/Inclusion-imaging-using-single-shot-ultrasound-and-convolutional-neural-networks.pdf" target="_blank">View Paper</a>
    </li>
    <li>
      <strong>Title:</strong> Video and Text Summarization Using Deep Learning<br>
      <strong>Venue:</strong> WACV 2017, Santa Rosa, CA<br>
      <strong>Description:</strong> This work introduces novel methods for summarizing and annotating long consumer videos by extracting impactful segments‚Äîbased on image quality, cinematography, and user preferences‚Äîand generating both visual summaries and textual descriptions via recurrent neural networks. Unlike traditional keyframe-only approaches, it combines visual subshot selection with sequential encoding to produce rich, multi-modal video summaries.<br>
      <strong>Link:</strong> <a href="https://www.researchgate.net/profile/Shagan-Sah/publication/316948434_Semantic_Text_Summarization_of_Long_Videos/links/5eecc89c458515814a6ae5fb/Semantic-Text-Summarization-of-Long-Videos.pdf" target="_blank">View Paper</a>
    </li>
    <li>
      <strong>Title:</strong> Salient Activity Recognition in Video Sequences<br>
      <strong>Venue:</strong> ICPR 2016, Canc√∫n, M√©xico <br>
      <strong>Description:</strong> This paper explores efficient and accurate methods for large-scale activity recognition in surveillance videos, addressing the challenges of processing vast amounts of streaming data. It presents a novel deep learning fusion architecture leveraging multi-modal inputs, including color spaces and optical flow, and introduces keyframe extraction to reduce computational load while maintaining classification performance.<br>
      <strong>Link:</strong> <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/0516.pdf" target="_blank">View Paper</a>
    </li>
  </ul>
</section>

<section id="talks">
  <h2>Talks & Presentations</h2>
  <ul>
    <li>When Algorithms Fall Short : Handling Real World Variability, IEEE Seattle, 2024, Seattle</li>
    <li>Building AI Solutions that fit the jigsaw puzzle, Innovation Renaissance, 2024, Seattle</li>
    <li>Deep Learning for Pneumonia, CVPR, DL-UIA, 2023, Vancouver</li>
    <li>Introduction to Neural Networks, IEEE Global Humanitarian Technology Conference, 2020</li>
    
  </ul>
</section>

<section id="contact">
  <h2>Contact</h2>
  <p>Email: <a href="mailto:sourabh.kulhare06@gmail.com">sourabh.kulhare06@gmail.com</a></p>
  <p>Location: Seattle, USA</p>
  <p>
    LinkedIn: <a href="https://www.linkedin.com/in/skrealworld/" target="_blank">Sourabh Kulhare</a><br>
    GitHub: <a href="https://github.com/skulhare" target="_blank">skulhare</a><br>
    Google Scholar: <a href="https://scholar.google.com/citations?user=hTZnzOQAAAAJ&hl=en" target="_blank">Sourabh Kulhare</a>
  </p>
  <a href="supporting_files/Sourabh-KulhareJune25.pdf" download class="resume-btn">üìÑ Download My Resume</a>
</section>

<footer>
  <p>¬© 2025 Sourabh Kulhare | Built with HTML</p>
</footer>

<button class="back-to-top" id="backToTop" onclick="scrollToTop()">
  ‚Üë
</button>

<script>
  // Back to top button functionality
  window.addEventListener('scroll', function() {
    const backToTop = document.getElementById('backToTop');
    if (window.pageYOffset > 300) {
      backToTop.classList.add('visible');
    } else {
      backToTop.classList.remove('visible');
    }
  });

  function scrollToTop() {
    window.scrollTo({
      top: 0,
      behavior: 'smooth'
    });
  }

  // Smooth scrolling for navigation links
  document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
      e.preventDefault();
      const target = document.querySelector(this.getAttribute('href'));
      if (target) {
        target.scrollIntoView({
          behavior: 'smooth',
          block: 'start'
        });
      }
    });
  });

  // Add entrance animations
  const observerOptions = {
    threshold: 0.1,
    rootMargin: '0px 0px -50px 0px'
  };

  const observer = new IntersectionObserver(function(entries) {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        entry.target.style.opacity = '1';
        entry.target.style.transform = 'translateY(0)';
      }
    });
  }, observerOptions);

  // Observe all sections for animation
  document.addEventListener('DOMContentLoaded', function() {
    const sections = document.querySelectorAll('section');
    sections.forEach(section => {
      section.style.opacity = '0';
      section.style.transform = 'translateY(20px)';
      section.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
      observer.observe(section);
    });
  });
</script>

</body>
</html>
